{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b70b34e",
   "metadata": {},
   "source": [
    "### Basic library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31019bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8911e33",
   "metadata": {},
   "source": [
    "### Read Dataset and Preprocessing of Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2ff4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "# Setting up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "\n",
    "def common_mistake(unit):\n",
    "    # Assuming the definition from your previous implementation\n",
    "    if unit in allowed_units:\n",
    "        return unit\n",
    "    if unit.replace('ter', 'tre') in allowed_units:\n",
    "        return unit.replace('ter', 'tre')\n",
    "    if unit.replace('feet', 'foot') in allowed_units:\n",
    "        return unit.replace('feet', 'foot')\n",
    "    if unit == 'lbs':\n",
    "        return 'pound'  # Fix 'lbs' to 'pound'\n",
    "    return unit\n",
    "\n",
    "def create_placeholder_image(image_save_path):\n",
    "    try:\n",
    "        placeholder_image = Image.new('RGB', (100, 100), color='black')\n",
    "        placeholder_image.save(image_save_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error creating placeholder image: {e}\")\n",
    "\n",
    "def download_image(image_link, save_folder, retries=3, delay=3):\n",
    "    if not isinstance(image_link, str):\n",
    "        return False\n",
    "\n",
    "    filename = os.path.basename(image_link)\n",
    "    image_save_path = os.path.join(save_folder, filename)\n",
    "\n",
    "    if os.path.exists(image_save_path):\n",
    "        return True\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            urllib.request.urlretrieve(image_link, image_save_path)\n",
    "            # Check if the downloaded image is valid\n",
    "            with Image.open(image_save_path) as img:\n",
    "                img.verify()  # Check for corrupted images\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"Failed to download {image_link} (Attempt {attempt+1}/{retries}): {e}\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "    # If download fails, create a placeholder image\n",
    "    logging.error(f\"Failed to download {image_link} after {retries} attempts. Creating placeholder.\")\n",
    "    create_placeholder_image(image_save_path)\n",
    "    return False\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Explore the data\n",
    "print(train_df.info())\n",
    "print(train_df.describe())\n",
    "sns.pairplot(train_df)\n",
    "plt.show()\n",
    "\n",
    "# Correlation analysis\n",
    "numeric_columns = train_df.select_dtypes(include=[np.number])\n",
    "correlation_matrix = numeric_columns.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.show()\n",
    "\n",
    "# Feature engineering (example: creating a new feature based on entity_name)\n",
    "def categorize_entity_name(name):\n",
    "    # Example categorization function assuming names are separated by underscores\n",
    "    if isinstance(name, str):\n",
    "        parts = name.split('_')\n",
    "        return parts[0] if len(parts) > 0 else 'Unknown'\n",
    "    return 'Unknown'\n",
    "\n",
    "train_df['entity_name_category'] = train_df['entity_name'].apply(lambda x: categorize_entity_name(x))\n",
    "\n",
    "# Preprocess the data (with additional handling for missing values)\n",
    "def preprocess_data(df):\n",
    "    df['entity_value'] = df['entity_value'].fillna('')\n",
    "    df['value'] = df['entity_value'].apply(lambda x: x.split()[0] if x else np.nan)\n",
    "    df['unit'] = df['entity_value'].apply(lambda x: x.split()[1] if x and len(x.split()) > 1 else np.nan)\n",
    "    return df\n",
    "\n",
    "train_df = preprocess_data(train_df)\n",
    "\n",
    "# Sample a subset of images\n",
    "num_images_to_download = 20000  # Specify the number of images to download\n",
    "sampled_df = train_df.sample(n=num_images_to_download, random_state=1)  # Sample random subset\n",
    "\n",
    "# Download images\n",
    "image_links = sampled_df['image_link'].dropna().tolist()  # Extract image links\n",
    "download_folder = 'data/images'  # Path where images will be saved\n",
    "\n",
    "if not os.path.exists(download_folder):\n",
    "    os.makedirs(download_folder)\n",
    "\n",
    "total_links = len(image_links)\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "\n",
    "for idx, image_link in enumerate(image_links):\n",
    "    if download_image(image_link, download_folder):\n",
    "        success_count += 1\n",
    "    else:\n",
    "        fail_count += 1\n",
    "\n",
    "    # Report progress\n",
    "    progress_percentage = (idx + 1) / total_links * 100\n",
    "    logging.info(f\"Progress: {progress_percentage:.2f}% - Success: {success_count} - Failed: {fail_count}\")\n",
    "\n",
    "# Save the preprocessed data with index\n",
    "train_df.to_csv('data/preprocessed_train.csv', index=True, index_label='index')\n",
    "\n",
    "print(\"Preprocessed data saved with index.\")\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50111d73",
   "metadata": {},
   "source": [
    "## Check the Number of Images in the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e962a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_images(image_dir):\n",
    "    # List all files in the directory\n",
    "    files = os.listdir(image_dir)\n",
    "\n",
    "    # Count files with common image extensions\n",
    "    image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff')\n",
    "    image_files = [f for f in files if f.lower().endswith(image_extensions)]\n",
    "\n",
    "    return len(image_files)\n",
    "\n",
    "image_dir = 'data/images'  # Change this to the path of your image directory\n",
    "num_images = count_images(image_dir)\n",
    "print(f\"Number of images in the directory '{image_dir}': {num_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76384753",
   "metadata": {},
   "source": [
    "## Correct the Image Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5925113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def correct_image_path(path):\n",
    "    # Remove any leading directories\n",
    "    filename = os.path.basename(path)\n",
    "    \n",
    "    # Add .jpg extension if missing\n",
    "    if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "        filename += '.jpg'\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    filename = filename.lower()\n",
    "    \n",
    "    return filename\n",
    "\n",
    "# Apply the correction to the DataFrame\n",
    "test_df['corrected_image_path'] = test_df['image_link'].apply(correct_image_path)\n",
    "\n",
    "# Check the results\n",
    "print(\"Original vs Corrected paths:\")\n",
    "print(test_df[['image_link', 'corrected_image_path']].head())\n",
    "\n",
    "# Check if the corrected paths match the files in the directory\n",
    "image_dir = 'data/images'\n",
    "dir_files = set(os.listdir(image_dir))\n",
    "corrected_paths = set(test_df['corrected_image_path'])\n",
    "\n",
    "matching_files = dir_files.intersection(corrected_paths)\n",
    "print(f\"\\nNumber of matching files after correction: {len(matching_files)}\")\n",
    "\n",
    "if len(matching_files) > 0:\n",
    "    print(\"First few matching files:\")\n",
    "    print(list(matching_files)[:5])\n",
    "else:\n",
    "    print(\"Still no matching files. Let's investigate further.\")\n",
    "    print(\"\\nFirst few files in directory:\")\n",
    "    print(list(dir_files)[:5])\n",
    "    print(\"\\nFirst few corrected paths:\")\n",
    "    print(list(corrected_paths)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e955efb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf6d3c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a1e22201",
   "metadata": {},
   "source": [
    "## Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e1be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class EntityValueExtractor(nn.Module):\n",
    "    def __init__(self, num_classes, num_units):\n",
    "        super(EntityValueExtractor, self).__init__()\n",
    "        self.base_model = models.resnet50(pretrained=True)\n",
    "        in_features = self.base_model.fc.in_features\n",
    "        self.base_model.fc = nn.Identity()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, 512)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc_value = nn.Linear(512, 1)  # Regression for the value\n",
    "        self.fc_unit = nn.Linear(512, num_units)  # Classification for the unit\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.base_model(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        value = self.fc_value(x)\n",
    "        unit = self.fc_unit(x)\n",
    "        return value, unit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b2bae0",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd3f091",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from src.model import EntityValueExtractor\n",
    "from data_loader import get_data_loaders\n",
    "from constants import allowed_units\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import logging\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm  # Import tqdm for progress bars\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s: %(message)s')\n",
    "\n",
    "# Load preprocessed data\n",
    "train_df = pd.read_csv('data/preprocessed_train.csv', low_memory=False)\n",
    "\n",
    "# Data validation\n",
    "print(\"DataFrame shape:\", train_df.shape)\n",
    "print(\"Columns:\", train_df.columns)\n",
    "print(\"Sample data:\")\n",
    "print(train_df.head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Updated EntityValueDataset class\n",
    "class EntityValueDataset(Dataset):\n",
    "    def __init__(self, df, image_dir, transform=None): # Fixed the typo here\n",
    "        self.df = df\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self): # Fixed the typo here\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx): # Fixed the typo here\n",
    "        image_filename = self.df.iloc[idx]['image_link'].split('/')[-1]\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "\n",
    "        try:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "        except FileNotFoundError:\n",
    "            logging.warning(f\"Warning: Image not found: {image_path}\")\n",
    "            return None\n",
    "\n",
    "        try:\n",
    "            value = float(self.df.iloc[idx]['value'])\n",
    "            value = torch.tensor(value, dtype=torch.float)\n",
    "        except ValueError:\n",
    "            logging.warning(f\"Invalid value encountered at index {idx}, using default value.\")\n",
    "            value = torch.tensor(0.0, dtype=torch.float)\n",
    "\n",
    "        unit = self.df.iloc[idx]['unit']\n",
    "        allowed_units_list = list(allowed_units)  # Convert set to list\n",
    "        if unit in allowed_units_list:\n",
    "            unit = torch.tensor(allowed_units_list.index(unit), dtype=torch.long)\n",
    "        else:\n",
    "            logging.warning(f\"Invalid unit encountered at index {idx}, using default unit.\")\n",
    "            unit = torch.tensor(0, dtype=torch.long)\n",
    "\n",
    "        return image, value, unit\n",
    "\n",
    "# Create data loader\n",
    "def get_data_loaders(df, image_dir, batch_size, num_workers):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    dataset = EntityValueDataset(df, image_dir, transform=transform)\n",
    "\n",
    "    def collate_fn(batch):\n",
    "        # Filter out None batches due to missing images\n",
    "        batch = list(filter(lambda x: x is not None, batch))\n",
    "        if len(batch) > 0:\n",
    "            return torch.utils.data.dataloader.default_collate(batch)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "    return loader\n",
    "\n",
    "train_loader = get_data_loaders(train_df, 'data/images', batch_size=64, num_workers=4)  # Increased batch size and num_workers\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EntityValueExtractor(num_classes=1, num_units=len(allowed_units)).to(device)\n",
    "value_criterion = nn.MSELoss()\n",
    "unit_criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scaler = GradScaler()  # Mixed precision scaler\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Using tqdm for progress bar\n",
    "    for i, batch in enumerate(tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', ncols=100)):\n",
    "        if batch is None or len(batch) == 0:\n",
    "            continue\n",
    "\n",
    "        images, values, units = batch\n",
    "        images, values, units = images.to(device), values.to(device), units.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():  # Mixed precision context\n",
    "            value_pred, unit_pred = model(images)\n",
    "            value_loss = value_criterion(value_pred.squeeze(), values)\n",
    "            unit_loss = unit_criterion(unit_pred, units)\n",
    "            loss = value_loss + unit_loss\n",
    "\n",
    "        scaler.scale(loss).backward()  # Scale loss and backward pass\n",
    "        scaler.step(optimizer)  # Update parameters\n",
    "        scaler.update()  # Update scaler\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "        if i % 1000 == 0:  # Log every 1000 batches\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Batch {i}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    avg_loss = train_loss / num_batches if num_batches > 0 else 0\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Train Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c61eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4de77525",
   "metadata": {},
   "source": [
    "## Save the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260c4344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be92d723",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba74e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "from src.model import EntityValueExtractor\n",
    "from data_loader import val_transform\n",
    "from constants import allowed_units\n",
    "\n",
    "# Define the dataset class to handle both local paths and URLs\n",
    "class ProductImageDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.valid_images = []\n",
    "\n",
    "        print(f\"DataFrame shape: {self.df.shape}\")\n",
    "        \n",
    "        # Ensure the 'image_link' column exists\n",
    "        if 'image_link' not in self.df.columns:\n",
    "            raise KeyError(\"'image_link' column is missing in the DataFrame\")\n",
    "        \n",
    "        print(\"First few image links:\")\n",
    "        print(self.df['image_link'].head())\n",
    "        print(f\"Image directory: {self.img_dir}\")\n",
    "\n",
    "        for idx, row in self.df.iterrows():\n",
    "            img_path = row['image_link']\n",
    "\n",
    "            # Check if the path is a URL or local path\n",
    "            if img_path.startswith('http'):\n",
    "                try:\n",
    "                    response = requests.get(img_path)\n",
    "                    if response.status_code == 200:\n",
    "                        self.valid_images.append(idx)\n",
    "                    else:\n",
    "                        print(f\"Failed to access URL: {img_path}\")\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Error fetching image from URL {img_path}: {e}\")\n",
    "            else:\n",
    "                local_img_path = os.path.join(self.img_dir, img_path)\n",
    "                if os.path.exists(local_img_path):\n",
    "                    self.valid_images.append(idx)\n",
    "                else:\n",
    "                    print(f\"Image not found: {local_img_path}\")\n",
    "\n",
    "            if idx % 1000 == 0:\n",
    "                print(f\"Processed {idx} images...\")\n",
    "\n",
    "        print(f\"Total valid images found: {len(self.valid_images)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_idx = self.valid_images[idx]\n",
    "        img_path = self.df.iloc[img_idx]['image_link']\n",
    "\n",
    "        # Check if the path is a URL or local path\n",
    "        if img_path.startswith('http'):\n",
    "            try:\n",
    "                response = requests.get(img_path)\n",
    "                image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image from URL {img_path}: {e}\")\n",
    "                raise\n",
    "        else:\n",
    "            local_img_path = os.path.join(self.img_dir, img_path)\n",
    "            try:\n",
    "                image = Image.open(local_img_path).convert('RGB')\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image at {local_img_path}: {e}\")\n",
    "                raise\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image\n",
    "\n",
    "# Load the model\n",
    "model = EntityValueExtractor(num_classes=1, num_units=len(allowed_units))\n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Load test data\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Create the dataset with image links (both URLs and local paths)\n",
    "test_dataset = ProductImageDataset(test_df, 'data/images', transform=val_transform)\n",
    "\n",
    "# Check how many valid images were found\n",
    "print(f\"Length of test_dataset: {len(test_dataset)}\")\n",
    "\n",
    "# Create DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "# Check if the DataLoader is empty\n",
    "if len(test_loader) == 0:\n",
    "    print(\"test_loader is empty. Check ProductImageDataset\")\n",
    "    print(f\"Length of test_df: {len(test_df)}\")\n",
    "    print(f\"First few image links in test_df: {test_df['image_link'].head()}\")\n",
    "else:\n",
    "    # Make predictions (only if test_loader is not empty)\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i, images in enumerate(test_loader):\n",
    "            print(f\"Processing batch {i+1}/{len(test_loader)}\")\n",
    "            images = images.to(model.base_model.fc.weight.device)\n",
    "            value_pred, unit_pred = model(images)\n",
    "\n",
    "            value_pred = value_pred.squeeze().cpu().numpy()\n",
    "            unit_pred = unit_pred.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            for v, u in zip(value_pred, unit_pred):\n",
    "                predictions.append(f\"{v:.2f} {allowed_units[u]}\")\n",
    "\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Current number of predictions: {len(predictions)}\")\n",
    "\n",
    "    print(f\"Total number of predictions: {len(predictions)}\")\n",
    "    print(f\"Number of rows in test_df: {len(test_df)}\")\n",
    "\n",
    "    # Create submission file\n",
    "    submission = pd.DataFrame({\n",
    "        'index': test_df['index'],\n",
    "        'prediction': predictions\n",
    "    })\n",
    "    submission.to_csv('output/test_out.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2dd8ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60ebd689",
   "metadata": {},
   "source": [
    "### Run Sanity check using src/sanity.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81bb3988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing successfull for file: ../dataset/sample_test_out.csv\n"
     ]
    }
   ],
   "source": [
    "!python sanity.py --test_filename ./data/sample_test.csv --output_filename ./data/sample_test_out.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aa79459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing successfull for file: ../dataset/sample_test_out_fail.csv\n"
     ]
    }
   ],
   "source": [
    "!python sanity.py --test_filename ./data/sample_test.csv --output_filename ./data/sample_test_out.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c76467",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c38a641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
